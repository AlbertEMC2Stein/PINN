<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>PDESolver.Solver API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>PDESolver.Solver</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.transforms import blended_transform_factory
from scipy.stats import gaussian_kde
from tqdm import tqdm
import warnings
import logging
import os

__all__ = [&#39;Solver&#39;, &#39;Optimizer&#39;]       

warnings.filterwarnings(&#34;ignore&#34;, category=UserWarning)
logging.getLogger(&#39;tensorflow&#39;).setLevel(logging.ERROR)
os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39;


class Solver:
    def __init__(self, bvp, optimizer, num_hidden_layers=4, num_neurons_per_layer=50):
        &#34;&#34;&#34;
        Constructor for a boundary value problem solver.

        Parameters
        -----------
        bvp: BoundaryValueProblem
            Boundary value problem to be solved
        optimizer: Optimizer
            Optimizer to be used for training the neural network
        num_hidden_layers: int
            Number of hidden layers in the neural network
            Defaults to 4
        num_neurons_per_layer:  int
            Number of neurons in each hidden layer
            Defaults to 50

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; optimizer = Optimizer(initial_learning_rate=1e-3, decay_steps=1000, decay_rate=0.9)
        &gt;&gt;&gt; bvp = Laplace()
        &gt;&gt;&gt; solver = Solver(bvp, optimizer, num_hidden_layers=4, num_neurons_per_layer=50)
        &#34;&#34;&#34;

        num_inputs = len(bvp.get_specification()[&#39;variables&#39;])
        num_outputs = len(bvp.get_specification()[&#39;components&#39;])
        inner_constraint = [condition for condition in bvp.get_conditions() if condition.name == &#39;inner&#39;][0]
        mean, variance = inner_constraint.get_normalization_constants()

        self.model = init_model(num_inputs, num_outputs, num_hidden_layers, num_neurons_per_layer, mean, variance)
        self.bvp = bvp
        self.optimizer = optimizer

        self.loss_history = []
        self.weight_history = [{cond.name: 1. for cond in bvp.get_conditions()}]
        self.weights = tf.Variable(np.ones(len(bvp.get_conditions())), dtype=tf.float32)
        self.step = tf.Variable(0)

    def compute_differentials(self, samplePoints):
        &#34;&#34;&#34;
        Calculates the differentials of the model at the given points.

        Parameters
        -----------
        samplePoints: tensor
            Tensor of points to calculate the differentials at

        Returns
        -----------
        dict: Dictionary containing the differentials of the model needed for the bvp at the given points

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
        &gt;&gt;&gt; samples = tf.constant([[0, 0], [0, 0.5], [0, 1]])  
        &gt;&gt;&gt; solver.compute_differentials(samples) 
        {&#39;x&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[0.],
            [0.],
            [0.]], dtype=float32)&gt;, &#39;y&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[0. ],
            [0.5],
            [1. ]], dtype=float32)&gt;, &#39;u&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[-1.4298753 ],
            [-1.0829226 ],
            [-0.86016774]], dtype=float32)&gt;, &#39;u_x&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[0.7101533 ],
            [0.7047943 ],
            [0.45764494]], dtype=float32)&gt;, &#39;u_xx&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[-1.6525286],
            [-1.6283079],
            [-1.2708694]], dtype=float32)&gt;, &#39;u_y&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[0.71874154],
            [0.6189531 ],
            [0.2856549 ]], dtype=float32)&gt;, &#39;u_yy&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[-0.01344297],
            [-0.5869958 ],
            [-0.47277603]], dtype=float32)&gt;}
        &#34;&#34;&#34;

        specs = self.bvp.get_specification()

        stack = lambda *tensors: tf.concat(tensors, axis=1)
        component_funs = {component: lambda x, index=i: self.model(x)[:, index] 
                          for i, component in enumerate(specs[&#34;components&#34;])}

        gradient_dict = {}

        with tf.GradientTape(persistent=True) as tape:
            for i, variable in enumerate(specs[&#34;variables&#34;]):
                gradient_dict[variable] = samplePoints[:, i:i + 1]
                tape.watch(gradient_dict[variable])

            watched = [gradient_dict[variable] for variable in specs[&#34;variables&#34;]]
            for component in specs[&#34;components&#34;]:
                output = component_funs[component](tf.stack(watched, axis=1))
                gradient_dict[component] = tf.reshape(output, (-1, 1))
    
            for differential in specs[&#34;differentials&#34;]:
                component = differential.split(&#34;_&#34;)[0]
                variables = differential.split(&#34;_&#34;)[1]

                differential_head = component 
                for i, variable in enumerate(variables):
                    if i == 0:
                        differential_head_new = differential_head + &#34;_&#34; + variable
                    else:
                        differential_head_new = differential_head + variable

                    if not differential_head_new in gradient_dict:
                        gradient_dict[differential_head_new] = tape.gradient(gradient_dict[differential_head], gradient_dict[variable])
                        differential_head = differential_head_new

        for (stacked_component_name, components) in specs[&#34;stacked_components&#34;].items():
            stacked = stack(*[gradient_dict[component] for component in components])
            gradient_dict[stacked_component_name] = stacked

        return gradient_dict
   
    def compute_residuals(self):
        &#34;&#34;&#34;
        Gets the residuals for the boundary value problem.

        Returns
        -----------
        dict: Dictionary of residuals of form {condition_name: residual}

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; solver = Solver(Laplace(minibatch_size=3), Optimizer()) 
        &gt;&gt;&gt; solver.compute_residuals()
        {&#39;zero_boundary&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[1.7787822],
            [2.6922252],
            [1.3558891]], dtype=float32)&gt;, &#39;f_boundary&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[2.4186134],
            [2.4582624],
            [2.4142814]], dtype=float32)&gt;, &#39;inner&#39;: &lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=
        array([[ 0.53346014],
            [-4.9377484 ],
            [ 0.21452093],
            [-1.6314204 ]], dtype=float32)&gt;}
        &#34;&#34;&#34;

        conditions = self.bvp.get_conditions()

        residuals = {}
        for condition in conditions:
            samples = condition.sample_points()
            Du = self.compute_differentials(samples)
            residuals[condition.name] = condition.residue_fn(Du)

        return residuals
  
    def compute_losses(self):
        &#34;&#34;&#34;
        Computes the losses for the neural network.
        
        Returns
        -----------
        dict: Dictionary of losses of form {condition_name: loss}

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer()) 
        &gt;&gt;&gt; solver.compute_losses() 
        {&#39;zero_boundary&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.29857153&gt;, &#39;f_boundary&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.07588613&gt;, &#39;inner&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.3834033&gt;, &#39;L2&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.757861&gt;, &#39;total&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=13.515722&gt;}
        &#34;&#34;&#34;

        criterion = tf.keras.losses.MeanSquaredError()
        residuals = self.compute_residuals()

        losses = {name: 0.0 for name in residuals.keys()}
        l2loss = 0.0
        for i, (name, residual) in enumerate(residuals.items()):
            losses[name] += self.weights[i] * criterion(residual, 0.0)
            l2loss += criterion(residual, 0.0)

        losses[&#39;total&#39;] = sum(losses.values())
        losses[&#39;L2&#39;] = l2loss

        return losses
 
    def compute_gradients(self):
        &#34;&#34;&#34;
        Gets the gradients of the neural network.

        Returns
        -----------
        tensor: L2 loss
        dict: Dictionary of gradients of form {condition_name: gradient}

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
        &gt;&gt;&gt; solver.compute_gradients()
        (&lt;tf.Tensor: shape=(), dtype=float32, numpy=4.756295&gt;, LARGE DICTIONARY)
        &#34;&#34;&#34;
        
        with tf.GradientTape(persistent=True) as tape:
            tape.watch(self.model.trainable_variables)
            losses = self.compute_losses()

            kwargs = {&#39;sources&#39;: self.model.trainable_variables, &#39;unconnected_gradients&#39;: tf.UnconnectedGradients.ZERO}
            grads = {name: tape.gradient(loss, **kwargs) for name, loss in losses.items() if name != &#39;L2&#39;}

        return losses[&#39;L2&#39;], grads


    def adjust_weights(self, gradients):
        &#34;&#34;&#34;
        Adjusts the weights of the PDE and data losses.

        Parameters
        -----------
        gradients: dict
            Dictionary of gradients as returned by compute_gradients()

        Returns
        -----------
        dict: Dictionary of adjusted condition weights of form {condition_name: weight}

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer()) 
        &gt;&gt;&gt; l2, gradients = solver.compute_gradients()
        &gt;&gt;&gt; solver.adjust_weights(gradients)
        {&#39;zero_boundary&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=38.457893&gt;, &#39;f_boundary&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=22.964037&gt;, &#39;inner&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;}
        &#34;&#34;&#34;

        gradient_vectors = [tf.concat([tf.reshape(gradient, [-1]) for gradient in gradients_list], axis=0) for gradients_list in gradients.values()][:-1] # excluding &#39;totalgrad&#39;
        variances = [tf.math.reduce_variance(gradient) for gradient in gradient_vectors]
        most_varying = tf.math.argmax(variances)
        most_varying_absmax = tf.math.reduce_max(tf.abs(tf.gather(gradient_vectors, most_varying)))

        new_weights = {}
        minimal = tf.float32.max
        for i, cond in enumerate(self.bvp.get_conditions()):
            name = cond.name
            if i != most_varying:
                new_weight = most_varying_absmax / (tf.reduce_mean(tf.abs(gradient_vectors[i])))
                new_weight = 0.9 * self.weights[i] + 0.1 * new_weight
            else:
                new_weight = self.weights[i]

            new_weights[name] = new_weight
            minimal = tf.math.minimum(minimal, new_weights[name])

        new_weights = {name: new_weights[name] / minimal for name in new_weights.keys()}
        self.weights.assign([new_weight for new_weight in new_weights.values()])
                
        return new_weights
    
    @tf.function
    def train_step(self):
        &#34;&#34;&#34;
        Performs a single training step.

        Returns
        -----------
        tensor: L2 loss
        dict: Dictionary of gradients of form {condition_name: gradient}
        dict: Dictionary of adjusted condition weights of form {condition_name: weight}

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
        &gt;&gt;&gt; l2loss, gradients, new_weights = solver.train_step()
        &gt;&gt;&gt; l2loss
        &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.371607&gt;
        &gt;&gt;&gt; l2loss, gradients, new_weights = solver.train_step()
        &gt;&gt;&gt; l2loss
        &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0963678&gt;
        &gt;&gt;&gt; l2loss, gradients, new_weights = solver.train_step()
        &gt;&gt;&gt; l2loss
        &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.029224&gt;
        &#34;&#34;&#34;
        
        l2loss, gradients = self.compute_gradients()
        
        if self.step % 10 == 0:
            new_weights = self.adjust_weights(gradients)  
        else:
            new_weights = {cond.name: -1. for cond in self.bvp.get_conditions()}          
            
        self.optimizer.apply_gradients(zip(gradients[&#39;total&#39;], self.model.trainable_variables))
        self.step.assign(self.step + 1)
        
        return l2loss, gradients, new_weights

    def train(self, iterations=10000, debug_frequency=2500):
        &#34;&#34;&#34;
        Trains the neural network to solve the boundary value problem.

        Parameters
        -----------
        iterations: int (default=10000)
            Number of iterations to train for
        debug_frequency: int (default=2500)
            Frequency (every X iterations) at which to show debug panel.
            If negative, no debug panel is shown

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
        &gt;&gt;&gt; solver.train(iterations=10000, debug_frequency=2500)
        øL²-loss = 6.044e+01 (best: 2.305e+01, 000064it ago) lr = 0.00090:   3%|█                                | 1271/40000 [00:33&lt;07:53, 81.76it/s]
        &#34;&#34;&#34;

        best_loss = np.inf
        iterations_since_last_improvement = 0
        k_max = int(np.ceil(np.log10(iterations))) + 1
        pbar = tqdm(range(iterations), desc=&#39;Pending...&#39;)

        for i in pbar:
            l2loss, gradients, new_weights = self.train_step()
            
            self.loss_history += [l2loss.numpy()]
            
            if l2loss.numpy() &lt; best_loss:
                best_loss = l2loss.numpy()
                iterations_since_last_improvement = 0
            else:
                iterations_since_last_improvement += 1
            
            if list(new_weights.values())[0] != -1:
                self.weight_history += [{name: new_weights[name].numpy() for name in new_weights.keys()}]
            
            avg_loss = np.mean(self.loss_history[-100:])
            pbar.desc = f&#39;øL²-loss = {avg_loss:.3e} (best: {best_loss:.3e}, {iterations_since_last_improvement:0{k_max}d}it ago) lr = {self.optimizer.lr.numpy():.5f}&#39;

            if debug_frequency &gt; 0 and (i % debug_frequency == 0 or i == iterations - 1):
                self.show_debugplot(gradients)

    def show_debugplot(self, gradients):
        &#34;&#34;&#34;
        Shows a debug plot of the neural network.

        Parameters
        -----------
        gradients: tuple
            Tuple of gradients of the PDE and data losses obtained from compute_gradients

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *                        
        &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
        &gt;&gt;&gt; l2loss, gradients = solver.compute_gradients()
        &gt;&gt;&gt; solver.loss_history += [l2loss.numpy()]
        &gt;&gt;&gt; solver.show_debugplot(gradients)    
        &#34;&#34;&#34;

        fig = plt.figure(figsize=(16, 8), layout=&#39;compressed&#39;)
        subfigs = fig.subfigures(2, 1, hspace=0)

        axs = subfigs[0].subplots(1, len(self.model.layers) - 4)
        subfigs[0].suptitle(&#39;Gradient Distributions&#39;)

        ax_count = -1
        for j in range(len(self.model.layers)):
            if j in [0, 1, 3, 4]:
                continue
            else:
                ax_count += 1

            layer_gradient = lambda name: tf.concat([tf.reshape(gradient, [-1]) for gradient in gradients[name][2*(j-2):2*(j-1)]], axis=0)
            density = lambda i, x: gaussian_kde(layer_gradient(i).numpy())(x)

            xs = np.linspace(-2.5, 2.5, 1000)
            for cond in self.bvp.get_conditions():
                axs[ax_count].plot(xs, density(cond.name, xs), lw=0.5, label=cond.name)

            axs[ax_count].set_xlim(min(xs), max(xs))
            axs[ax_count].set_ylim(0, 100)
            axs[ax_count].set_yscale(&#39;symlog&#39;)
            axs[ax_count].set_title(self.model.layers[j].name)

        axs[-1].legend()

        n = len(self.loss_history)
        averaged_loss = [np.mean(self.loss_history[:k+1] if k &lt; 99 else self.loss_history[k-99:k+1]) for k in range(n)]
        best_loss = np.min(self.loss_history)

        axs = subfigs[1].subplots(1, 2)
        trans = blended_transform_factory(axs[0].transAxes, axs[0].transData)
        loss_handle, = axs[0].semilogy(range(n), self.loss_history, &#39;k-&#39;, lw=0.5, alpha=0.5, label=&#39;Loss&#39;)
        avg_loss_handle, = axs[0].semilogy(range(n), averaged_loss, &#39;r--&#39;, lw=1, label=f&#39;$ø_{{{min(100, n)}}}$ Loss&#39;)
        axs[0].axhline(best_loss, color=&#39;g&#39;, lw=0.5)
        axs[0].text(0.995, 0.97 * best_loss, f&#39;Best: {best_loss:.3e}&#39;, ha=&#39;right&#39;, va=&#39;top&#39;, color=&#39;g&#39;, transform=trans)

        ax = axs[0].twinx()
        ax.set_yscale(&#39;log&#39;)
        lr_handle, = ax.plot(self.optimizer.lr_history_upto(n), &#39;b-&#39;, lw=0.5, label=&#39;Learning rate&#39;)

        axs[0].set_title(&#39;Loss (left) and learning rate (right) history&#39;)
        axs[0].set_xlabel(&#39;Iteration&#39;)
        axs[0].set_xlim(0, n - (1 if n &gt; 1 else 0))

        if n &gt; 1:
            handles = [loss_handle, avg_loss_handle, lr_handle]
            axs[0].legend(handles=handles, loc=&#39;lower left&#39;)

        for cond in self.bvp.get_conditions():
            cond_weight_history = [self.weight_history[k][cond.name] for k in range(len(self.weight_history))]
            axs[1].plot(cond_weight_history, lw=0.5)

        axs[1].set_title(&#39;Weight History&#39;)
        axs[1].set_xlabel(&#39;Update #&#39;)
        axs[1].set_xlim(0, len(self.weight_history) - 1)
        axs[1].set_yscale(&#39;log&#39;)

        if os.name == &#39;nt&#39;:
            figManager = plt.get_current_fig_manager()
            figManager.window.state(&#34;zoomed&#34;)
        
        plt.show()


class Optimizer(tf.keras.optimizers.Adam):
    &#34;&#34;&#34;&#34;&#34;&#34;
    def __init__(self, initial_learning_rate=1e-3, decay_steps=1000, decay_rate=0.9):
        &#34;&#34;&#34;
        Optimizer for the neural network used in a solver object.
        Acts as a wrapper for the Adam optimizer with an exponential learning rate decay.

        Parameters
        -----------
        initial_learning_rate: float
            Initial learning rate of the optimizer.
            Defaults to 1e-3.
        decay_steps: int
            Number of iterations after which the learning rate has decayed by the specified factor.
            Defaults to 1000.
        decay_rate: float
            Factor by which the learning rate is decayed.
            Defaults to 0.9.

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; optimizer = Optimizer(initial_learning_rate=1, decay_steps=500, decay_rate=0.5)
        &gt;&gt;&gt; solver = Solver(Laplace(), optimizer)
        &#34;&#34;&#34;

        self.lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate) 
        super().__init__(learning_rate=self.lr_scheduler)

    def lr_history_upto(self, iteration):
        &#34;&#34;&#34;
        Returns the learning rate history up to the specified iteration.

        Parameters
        -----------
        iteration: int
            Iteration up to which the learning rate history is returned.

        Returns
        -----------
        numpy.ndarray: Learning rate history up to the specified iteration.

        Examples
        -----------
        &gt;&gt;&gt; optimizer = Optimizer()
        &gt;&gt;&gt; optimizer.lr_history_upto(10)
        array([0.001     , 0.00099989, 0.00099979, 0.00099968, 0.00099958,
               0.00099947, 0.00099937, 0.00099926, 0.00099916, 0.00099905,
               0.00099895], dtype=float32)&gt;
        &#34;&#34;&#34;

        iters = np.arange(0, iteration + 1)
        return self.lr_scheduler(iters)
        

###################################################################################
###################################################################################
###################################################################################

                
def xavier_init(size):
    in_dim = size[0]
    out_dim = size[1]
    xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)
    return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,
                        dtype=tf.float32, trainable=True)


class Encoder(tf.keras.layers.Layer):
    def __init__(self, inputs, outputs, activation=tf.tanh):
        super(Encoder, self).__init__()
        self.inputs = inputs
        self.outputs = outputs
        self.activation = activation

    def build(self, input_shape):  
        self.W = xavier_init([self.inputs, self.outputs]) 
        self.b = xavier_init([1, self.outputs])

    def call(self, inputs):  
        return self.activation(tf.add(tf.matmul(inputs, self.W), self.b))
    

class ImprovedLinear(tf.keras.layers.Layer):
    def __init__(self, inputs, outputs, activation=tf.tanh):
        super(ImprovedLinear, self).__init__()
        self.inputs = inputs
        self.outputs = outputs
        self.activation = activation

    def build(self, input_shape):  
        self.W = xavier_init([self.inputs, self.outputs]) 
        self.b = xavier_init([1, self.outputs])

    def call(self, inputs, encoder_1, encoder_2):  
        return tf.math.multiply(self.activation(tf.add(tf.matmul(inputs, self.W), self.b)), encoder_1) + \
                tf.math.multiply(1 - self.activation(tf.add(tf.matmul(inputs, self.W), self.b)), encoder_2)


class Linear(tf.keras.layers.Layer):
    def __init__(self, inputs, outputs, activation=tf.tanh):
        super(Linear, self).__init__()
        self.inputs = inputs
        self.outputs = outputs
        self.activation = activation

    def build(self, input_shape):  
        self.W = xavier_init([self.inputs, self.outputs]) 
        self.b = xavier_init([1, self.outputs])

    def call(self, inputs):  
        return self.activation(tf.add(tf.matmul(inputs, self.W), self.b))


def init_model(num_inputs, num_outputs, num_hidden_layers, num_neurons_per_layer, mean, variance):
    layer_sizes = [num_inputs] + [num_neurons_per_layer] * num_hidden_layers + [num_outputs] 
    layers = [Linear(layer_sizes[0], layer_sizes[1])] + \
             [ImprovedLinear(layer_sizes[i], layer_sizes[i + 1]) for i in range(1, len(layer_sizes) - 2)] + \
             [Linear(layer_sizes[-2], layer_sizes[-1], activation=tf.identity)]

    encoder_1 = Encoder(num_inputs, num_neurons_per_layer)
    encoder_2 = Encoder(num_inputs, num_neurons_per_layer)

    inputs = tf.keras.Input(num_inputs)
    outputs = tf.keras.layers.Normalization(axis=-1, mean=mean, variance=variance)(inputs)

    E1 = encoder_1(outputs)
    E2 = encoder_2(outputs)  
    for i in range(len(layers) - 1):
        if i == 0:
            outputs = layers[i](outputs)
        else:
            outputs = layers[i](outputs, E1, E2)

    outputs = layers[-1](outputs)
    model = tf.keras.Model(inputs=inputs, outputs=outputs)

    return model


if __name__ == &#39;__main__&#39;:
    from Sampling import Cuboid, Equidistant

    tf.random.set_seed(1)
    txs = Cuboid([0, 0], [1, 1]).pick(9, Equidistant())

    model = init_model(2, 1, 4, 50, mean=-1, variance=1)

    model.summary()
    print(model(txs)) </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="PDESolver.Solver.Optimizer"><code class="flex name class">
<span>class <span class="ident">Optimizer</span></span>
<span>(</span><span>initial_learning_rate=0.001, decay_steps=1000, decay_rate=0.9)</span>
</code></dt>
<dd>
<div class="desc"><p>Optimizer for the neural network used in a solver object.
Acts as a wrapper for the Adam optimizer with an exponential learning rate decay.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>initial_learning_rate</code></strong> :&ensp;<code>float</code></dt>
<dd>Initial learning rate of the optimizer.
Defaults to 1e-3.</dd>
<dt><strong><code>decay_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of iterations after which the learning rate has decayed by the specified factor.
Defaults to 1000.</dd>
<dt><strong><code>decay_rate</code></strong> :&ensp;<code>float</code></dt>
<dd>Factor by which the learning rate is decayed.
Defaults to 0.9.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from PDESolver import *
&gt;&gt;&gt; optimizer = Optimizer(initial_learning_rate=1, decay_steps=500, decay_rate=0.5)
&gt;&gt;&gt; solver = Solver(Laplace(), optimizer)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Optimizer(tf.keras.optimizers.Adam):
    &#34;&#34;&#34;&#34;&#34;&#34;
    def __init__(self, initial_learning_rate=1e-3, decay_steps=1000, decay_rate=0.9):
        &#34;&#34;&#34;
        Optimizer for the neural network used in a solver object.
        Acts as a wrapper for the Adam optimizer with an exponential learning rate decay.

        Parameters
        -----------
        initial_learning_rate: float
            Initial learning rate of the optimizer.
            Defaults to 1e-3.
        decay_steps: int
            Number of iterations after which the learning rate has decayed by the specified factor.
            Defaults to 1000.
        decay_rate: float
            Factor by which the learning rate is decayed.
            Defaults to 0.9.

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; optimizer = Optimizer(initial_learning_rate=1, decay_steps=500, decay_rate=0.5)
        &gt;&gt;&gt; solver = Solver(Laplace(), optimizer)
        &#34;&#34;&#34;

        self.lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate) 
        super().__init__(learning_rate=self.lr_scheduler)

    def lr_history_upto(self, iteration):
        &#34;&#34;&#34;
        Returns the learning rate history up to the specified iteration.

        Parameters
        -----------
        iteration: int
            Iteration up to which the learning rate history is returned.

        Returns
        -----------
        numpy.ndarray: Learning rate history up to the specified iteration.

        Examples
        -----------
        &gt;&gt;&gt; optimizer = Optimizer()
        &gt;&gt;&gt; optimizer.lr_history_upto(10)
        array([0.001     , 0.00099989, 0.00099979, 0.00099968, 0.00099958,
               0.00099947, 0.00099937, 0.00099926, 0.00099916, 0.00099905,
               0.00099895], dtype=float32)&gt;
        &#34;&#34;&#34;

        iters = np.arange(0, iteration + 1)
        return self.lr_scheduler(iters)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.optimizers.optimizer_experimental.adam.Adam</li>
<li>keras.optimizers.optimizer_experimental.optimizer.Optimizer</li>
<li>keras.optimizers.optimizer_experimental.optimizer._BaseOptimizer</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PDESolver.Solver.Optimizer.lr_history_upto"><code class="name flex">
<span>def <span class="ident">lr_history_upto</span></span>(<span>self, iteration)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the learning rate history up to the specified iteration.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>iteration</code></strong> :&ensp;<code>int</code></dt>
<dd>Iteration up to which the learning rate history is returned.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>numpy.ndarray: Learning rate history up to the specified iteration.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; optimizer = Optimizer()
&gt;&gt;&gt; optimizer.lr_history_upto(10)
array([0.001     , 0.00099989, 0.00099979, 0.00099968, 0.00099958,
       0.00099947, 0.00099937, 0.00099926, 0.00099916, 0.00099905,
       0.00099895], dtype=float32)&gt;
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lr_history_upto(self, iteration):
    &#34;&#34;&#34;
    Returns the learning rate history up to the specified iteration.

    Parameters
    -----------
    iteration: int
        Iteration up to which the learning rate history is returned.

    Returns
    -----------
    numpy.ndarray: Learning rate history up to the specified iteration.

    Examples
    -----------
    &gt;&gt;&gt; optimizer = Optimizer()
    &gt;&gt;&gt; optimizer.lr_history_upto(10)
    array([0.001     , 0.00099989, 0.00099979, 0.00099968, 0.00099958,
           0.00099947, 0.00099937, 0.00099926, 0.00099916, 0.00099905,
           0.00099895], dtype=float32)&gt;
    &#34;&#34;&#34;

    iters = np.arange(0, iteration + 1)
    return self.lr_scheduler(iters)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="PDESolver.Solver.Solver"><code class="flex name class">
<span>class <span class="ident">Solver</span></span>
<span>(</span><span>bvp, optimizer, num_hidden_layers=4, num_neurons_per_layer=50)</span>
</code></dt>
<dd>
<div class="desc"><p>Constructor for a boundary value problem solver.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>bvp</code></strong> :&ensp;<code>BoundaryValueProblem</code></dt>
<dd>Boundary value problem to be solved</dd>
<dt><strong><code>optimizer</code></strong> :&ensp;<code><a title="PDESolver.Solver.Optimizer" href="#PDESolver.Solver.Optimizer">Optimizer</a></code></dt>
<dd>Optimizer to be used for training the neural network</dd>
<dt><strong><code>num_hidden_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of hidden layers in the neural network
Defaults to 4</dd>
<dt><strong><code>num_neurons_per_layer</code></strong> :&ensp;<code> int</code></dt>
<dd>Number of neurons in each hidden layer
Defaults to 50</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from PDESolver import *
&gt;&gt;&gt; optimizer = Optimizer(initial_learning_rate=1e-3, decay_steps=1000, decay_rate=0.9)
&gt;&gt;&gt; bvp = Laplace()
&gt;&gt;&gt; solver = Solver(bvp, optimizer, num_hidden_layers=4, num_neurons_per_layer=50)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Solver:
    def __init__(self, bvp, optimizer, num_hidden_layers=4, num_neurons_per_layer=50):
        &#34;&#34;&#34;
        Constructor for a boundary value problem solver.

        Parameters
        -----------
        bvp: BoundaryValueProblem
            Boundary value problem to be solved
        optimizer: Optimizer
            Optimizer to be used for training the neural network
        num_hidden_layers: int
            Number of hidden layers in the neural network
            Defaults to 4
        num_neurons_per_layer:  int
            Number of neurons in each hidden layer
            Defaults to 50

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; optimizer = Optimizer(initial_learning_rate=1e-3, decay_steps=1000, decay_rate=0.9)
        &gt;&gt;&gt; bvp = Laplace()
        &gt;&gt;&gt; solver = Solver(bvp, optimizer, num_hidden_layers=4, num_neurons_per_layer=50)
        &#34;&#34;&#34;

        num_inputs = len(bvp.get_specification()[&#39;variables&#39;])
        num_outputs = len(bvp.get_specification()[&#39;components&#39;])
        inner_constraint = [condition for condition in bvp.get_conditions() if condition.name == &#39;inner&#39;][0]
        mean, variance = inner_constraint.get_normalization_constants()

        self.model = init_model(num_inputs, num_outputs, num_hidden_layers, num_neurons_per_layer, mean, variance)
        self.bvp = bvp
        self.optimizer = optimizer

        self.loss_history = []
        self.weight_history = [{cond.name: 1. for cond in bvp.get_conditions()}]
        self.weights = tf.Variable(np.ones(len(bvp.get_conditions())), dtype=tf.float32)
        self.step = tf.Variable(0)

    def compute_differentials(self, samplePoints):
        &#34;&#34;&#34;
        Calculates the differentials of the model at the given points.

        Parameters
        -----------
        samplePoints: tensor
            Tensor of points to calculate the differentials at

        Returns
        -----------
        dict: Dictionary containing the differentials of the model needed for the bvp at the given points

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
        &gt;&gt;&gt; samples = tf.constant([[0, 0], [0, 0.5], [0, 1]])  
        &gt;&gt;&gt; solver.compute_differentials(samples) 
        {&#39;x&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[0.],
            [0.],
            [0.]], dtype=float32)&gt;, &#39;y&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[0. ],
            [0.5],
            [1. ]], dtype=float32)&gt;, &#39;u&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[-1.4298753 ],
            [-1.0829226 ],
            [-0.86016774]], dtype=float32)&gt;, &#39;u_x&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[0.7101533 ],
            [0.7047943 ],
            [0.45764494]], dtype=float32)&gt;, &#39;u_xx&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[-1.6525286],
            [-1.6283079],
            [-1.2708694]], dtype=float32)&gt;, &#39;u_y&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[0.71874154],
            [0.6189531 ],
            [0.2856549 ]], dtype=float32)&gt;, &#39;u_yy&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[-0.01344297],
            [-0.5869958 ],
            [-0.47277603]], dtype=float32)&gt;}
        &#34;&#34;&#34;

        specs = self.bvp.get_specification()

        stack = lambda *tensors: tf.concat(tensors, axis=1)
        component_funs = {component: lambda x, index=i: self.model(x)[:, index] 
                          for i, component in enumerate(specs[&#34;components&#34;])}

        gradient_dict = {}

        with tf.GradientTape(persistent=True) as tape:
            for i, variable in enumerate(specs[&#34;variables&#34;]):
                gradient_dict[variable] = samplePoints[:, i:i + 1]
                tape.watch(gradient_dict[variable])

            watched = [gradient_dict[variable] for variable in specs[&#34;variables&#34;]]
            for component in specs[&#34;components&#34;]:
                output = component_funs[component](tf.stack(watched, axis=1))
                gradient_dict[component] = tf.reshape(output, (-1, 1))
    
            for differential in specs[&#34;differentials&#34;]:
                component = differential.split(&#34;_&#34;)[0]
                variables = differential.split(&#34;_&#34;)[1]

                differential_head = component 
                for i, variable in enumerate(variables):
                    if i == 0:
                        differential_head_new = differential_head + &#34;_&#34; + variable
                    else:
                        differential_head_new = differential_head + variable

                    if not differential_head_new in gradient_dict:
                        gradient_dict[differential_head_new] = tape.gradient(gradient_dict[differential_head], gradient_dict[variable])
                        differential_head = differential_head_new

        for (stacked_component_name, components) in specs[&#34;stacked_components&#34;].items():
            stacked = stack(*[gradient_dict[component] for component in components])
            gradient_dict[stacked_component_name] = stacked

        return gradient_dict
   
    def compute_residuals(self):
        &#34;&#34;&#34;
        Gets the residuals for the boundary value problem.

        Returns
        -----------
        dict: Dictionary of residuals of form {condition_name: residual}

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; solver = Solver(Laplace(minibatch_size=3), Optimizer()) 
        &gt;&gt;&gt; solver.compute_residuals()
        {&#39;zero_boundary&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[1.7787822],
            [2.6922252],
            [1.3558891]], dtype=float32)&gt;, &#39;f_boundary&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
        array([[2.4186134],
            [2.4582624],
            [2.4142814]], dtype=float32)&gt;, &#39;inner&#39;: &lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=
        array([[ 0.53346014],
            [-4.9377484 ],
            [ 0.21452093],
            [-1.6314204 ]], dtype=float32)&gt;}
        &#34;&#34;&#34;

        conditions = self.bvp.get_conditions()

        residuals = {}
        for condition in conditions:
            samples = condition.sample_points()
            Du = self.compute_differentials(samples)
            residuals[condition.name] = condition.residue_fn(Du)

        return residuals
  
    def compute_losses(self):
        &#34;&#34;&#34;
        Computes the losses for the neural network.
        
        Returns
        -----------
        dict: Dictionary of losses of form {condition_name: loss}

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer()) 
        &gt;&gt;&gt; solver.compute_losses() 
        {&#39;zero_boundary&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.29857153&gt;, &#39;f_boundary&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.07588613&gt;, &#39;inner&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.3834033&gt;, &#39;L2&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.757861&gt;, &#39;total&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=13.515722&gt;}
        &#34;&#34;&#34;

        criterion = tf.keras.losses.MeanSquaredError()
        residuals = self.compute_residuals()

        losses = {name: 0.0 for name in residuals.keys()}
        l2loss = 0.0
        for i, (name, residual) in enumerate(residuals.items()):
            losses[name] += self.weights[i] * criterion(residual, 0.0)
            l2loss += criterion(residual, 0.0)

        losses[&#39;total&#39;] = sum(losses.values())
        losses[&#39;L2&#39;] = l2loss

        return losses
 
    def compute_gradients(self):
        &#34;&#34;&#34;
        Gets the gradients of the neural network.

        Returns
        -----------
        tensor: L2 loss
        dict: Dictionary of gradients of form {condition_name: gradient}

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
        &gt;&gt;&gt; solver.compute_gradients()
        (&lt;tf.Tensor: shape=(), dtype=float32, numpy=4.756295&gt;, LARGE DICTIONARY)
        &#34;&#34;&#34;
        
        with tf.GradientTape(persistent=True) as tape:
            tape.watch(self.model.trainable_variables)
            losses = self.compute_losses()

            kwargs = {&#39;sources&#39;: self.model.trainable_variables, &#39;unconnected_gradients&#39;: tf.UnconnectedGradients.ZERO}
            grads = {name: tape.gradient(loss, **kwargs) for name, loss in losses.items() if name != &#39;L2&#39;}

        return losses[&#39;L2&#39;], grads


    def adjust_weights(self, gradients):
        &#34;&#34;&#34;
        Adjusts the weights of the PDE and data losses.

        Parameters
        -----------
        gradients: dict
            Dictionary of gradients as returned by compute_gradients()

        Returns
        -----------
        dict: Dictionary of adjusted condition weights of form {condition_name: weight}

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer()) 
        &gt;&gt;&gt; l2, gradients = solver.compute_gradients()
        &gt;&gt;&gt; solver.adjust_weights(gradients)
        {&#39;zero_boundary&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=38.457893&gt;, &#39;f_boundary&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=22.964037&gt;, &#39;inner&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;}
        &#34;&#34;&#34;

        gradient_vectors = [tf.concat([tf.reshape(gradient, [-1]) for gradient in gradients_list], axis=0) for gradients_list in gradients.values()][:-1] # excluding &#39;totalgrad&#39;
        variances = [tf.math.reduce_variance(gradient) for gradient in gradient_vectors]
        most_varying = tf.math.argmax(variances)
        most_varying_absmax = tf.math.reduce_max(tf.abs(tf.gather(gradient_vectors, most_varying)))

        new_weights = {}
        minimal = tf.float32.max
        for i, cond in enumerate(self.bvp.get_conditions()):
            name = cond.name
            if i != most_varying:
                new_weight = most_varying_absmax / (tf.reduce_mean(tf.abs(gradient_vectors[i])))
                new_weight = 0.9 * self.weights[i] + 0.1 * new_weight
            else:
                new_weight = self.weights[i]

            new_weights[name] = new_weight
            minimal = tf.math.minimum(minimal, new_weights[name])

        new_weights = {name: new_weights[name] / minimal for name in new_weights.keys()}
        self.weights.assign([new_weight for new_weight in new_weights.values()])
                
        return new_weights
    
    @tf.function
    def train_step(self):
        &#34;&#34;&#34;
        Performs a single training step.

        Returns
        -----------
        tensor: L2 loss
        dict: Dictionary of gradients of form {condition_name: gradient}
        dict: Dictionary of adjusted condition weights of form {condition_name: weight}

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
        &gt;&gt;&gt; l2loss, gradients, new_weights = solver.train_step()
        &gt;&gt;&gt; l2loss
        &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.371607&gt;
        &gt;&gt;&gt; l2loss, gradients, new_weights = solver.train_step()
        &gt;&gt;&gt; l2loss
        &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0963678&gt;
        &gt;&gt;&gt; l2loss, gradients, new_weights = solver.train_step()
        &gt;&gt;&gt; l2loss
        &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.029224&gt;
        &#34;&#34;&#34;
        
        l2loss, gradients = self.compute_gradients()
        
        if self.step % 10 == 0:
            new_weights = self.adjust_weights(gradients)  
        else:
            new_weights = {cond.name: -1. for cond in self.bvp.get_conditions()}          
            
        self.optimizer.apply_gradients(zip(gradients[&#39;total&#39;], self.model.trainable_variables))
        self.step.assign(self.step + 1)
        
        return l2loss, gradients, new_weights

    def train(self, iterations=10000, debug_frequency=2500):
        &#34;&#34;&#34;
        Trains the neural network to solve the boundary value problem.

        Parameters
        -----------
        iterations: int (default=10000)
            Number of iterations to train for
        debug_frequency: int (default=2500)
            Frequency (every X iterations) at which to show debug panel.
            If negative, no debug panel is shown

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *
        &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
        &gt;&gt;&gt; solver.train(iterations=10000, debug_frequency=2500)
        øL²-loss = 6.044e+01 (best: 2.305e+01, 000064it ago) lr = 0.00090:   3%|█                                | 1271/40000 [00:33&lt;07:53, 81.76it/s]
        &#34;&#34;&#34;

        best_loss = np.inf
        iterations_since_last_improvement = 0
        k_max = int(np.ceil(np.log10(iterations))) + 1
        pbar = tqdm(range(iterations), desc=&#39;Pending...&#39;)

        for i in pbar:
            l2loss, gradients, new_weights = self.train_step()
            
            self.loss_history += [l2loss.numpy()]
            
            if l2loss.numpy() &lt; best_loss:
                best_loss = l2loss.numpy()
                iterations_since_last_improvement = 0
            else:
                iterations_since_last_improvement += 1
            
            if list(new_weights.values())[0] != -1:
                self.weight_history += [{name: new_weights[name].numpy() for name in new_weights.keys()}]
            
            avg_loss = np.mean(self.loss_history[-100:])
            pbar.desc = f&#39;øL²-loss = {avg_loss:.3e} (best: {best_loss:.3e}, {iterations_since_last_improvement:0{k_max}d}it ago) lr = {self.optimizer.lr.numpy():.5f}&#39;

            if debug_frequency &gt; 0 and (i % debug_frequency == 0 or i == iterations - 1):
                self.show_debugplot(gradients)

    def show_debugplot(self, gradients):
        &#34;&#34;&#34;
        Shows a debug plot of the neural network.

        Parameters
        -----------
        gradients: tuple
            Tuple of gradients of the PDE and data losses obtained from compute_gradients

        Examples
        -----------
        &gt;&gt;&gt; from PDESolver import *                        
        &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
        &gt;&gt;&gt; l2loss, gradients = solver.compute_gradients()
        &gt;&gt;&gt; solver.loss_history += [l2loss.numpy()]
        &gt;&gt;&gt; solver.show_debugplot(gradients)    
        &#34;&#34;&#34;

        fig = plt.figure(figsize=(16, 8), layout=&#39;compressed&#39;)
        subfigs = fig.subfigures(2, 1, hspace=0)

        axs = subfigs[0].subplots(1, len(self.model.layers) - 4)
        subfigs[0].suptitle(&#39;Gradient Distributions&#39;)

        ax_count = -1
        for j in range(len(self.model.layers)):
            if j in [0, 1, 3, 4]:
                continue
            else:
                ax_count += 1

            layer_gradient = lambda name: tf.concat([tf.reshape(gradient, [-1]) for gradient in gradients[name][2*(j-2):2*(j-1)]], axis=0)
            density = lambda i, x: gaussian_kde(layer_gradient(i).numpy())(x)

            xs = np.linspace(-2.5, 2.5, 1000)
            for cond in self.bvp.get_conditions():
                axs[ax_count].plot(xs, density(cond.name, xs), lw=0.5, label=cond.name)

            axs[ax_count].set_xlim(min(xs), max(xs))
            axs[ax_count].set_ylim(0, 100)
            axs[ax_count].set_yscale(&#39;symlog&#39;)
            axs[ax_count].set_title(self.model.layers[j].name)

        axs[-1].legend()

        n = len(self.loss_history)
        averaged_loss = [np.mean(self.loss_history[:k+1] if k &lt; 99 else self.loss_history[k-99:k+1]) for k in range(n)]
        best_loss = np.min(self.loss_history)

        axs = subfigs[1].subplots(1, 2)
        trans = blended_transform_factory(axs[0].transAxes, axs[0].transData)
        loss_handle, = axs[0].semilogy(range(n), self.loss_history, &#39;k-&#39;, lw=0.5, alpha=0.5, label=&#39;Loss&#39;)
        avg_loss_handle, = axs[0].semilogy(range(n), averaged_loss, &#39;r--&#39;, lw=1, label=f&#39;$ø_{{{min(100, n)}}}$ Loss&#39;)
        axs[0].axhline(best_loss, color=&#39;g&#39;, lw=0.5)
        axs[0].text(0.995, 0.97 * best_loss, f&#39;Best: {best_loss:.3e}&#39;, ha=&#39;right&#39;, va=&#39;top&#39;, color=&#39;g&#39;, transform=trans)

        ax = axs[0].twinx()
        ax.set_yscale(&#39;log&#39;)
        lr_handle, = ax.plot(self.optimizer.lr_history_upto(n), &#39;b-&#39;, lw=0.5, label=&#39;Learning rate&#39;)

        axs[0].set_title(&#39;Loss (left) and learning rate (right) history&#39;)
        axs[0].set_xlabel(&#39;Iteration&#39;)
        axs[0].set_xlim(0, n - (1 if n &gt; 1 else 0))

        if n &gt; 1:
            handles = [loss_handle, avg_loss_handle, lr_handle]
            axs[0].legend(handles=handles, loc=&#39;lower left&#39;)

        for cond in self.bvp.get_conditions():
            cond_weight_history = [self.weight_history[k][cond.name] for k in range(len(self.weight_history))]
            axs[1].plot(cond_weight_history, lw=0.5)

        axs[1].set_title(&#39;Weight History&#39;)
        axs[1].set_xlabel(&#39;Update #&#39;)
        axs[1].set_xlim(0, len(self.weight_history) - 1)
        axs[1].set_yscale(&#39;log&#39;)

        if os.name == &#39;nt&#39;:
            figManager = plt.get_current_fig_manager()
            figManager.window.state(&#34;zoomed&#34;)
        
        plt.show()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="PDESolver.Solver.Solver.adjust_weights"><code class="name flex">
<span>def <span class="ident">adjust_weights</span></span>(<span>self, gradients)</span>
</code></dt>
<dd>
<div class="desc"><p>Adjusts the weights of the PDE and data losses.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>gradients</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary of gradients as returned by compute_gradients()</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong> :&ensp;<code>Dictionary</code> of <code>adjusted condition weights</code> of <code>form {condition_name: weight}</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from PDESolver import *
&gt;&gt;&gt; solver = Solver(Laplace(), Optimizer()) 
&gt;&gt;&gt; l2, gradients = solver.compute_gradients()
&gt;&gt;&gt; solver.adjust_weights(gradients)
{'zero_boundary': &lt;tf.Tensor: shape=(), dtype=float32, numpy=38.457893&gt;, 'f_boundary': &lt;tf.Tensor: shape=(), dtype=float32, numpy=22.964037&gt;, 'inner': &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;}
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adjust_weights(self, gradients):
    &#34;&#34;&#34;
    Adjusts the weights of the PDE and data losses.

    Parameters
    -----------
    gradients: dict
        Dictionary of gradients as returned by compute_gradients()

    Returns
    -----------
    dict: Dictionary of adjusted condition weights of form {condition_name: weight}

    Examples
    -----------
    &gt;&gt;&gt; from PDESolver import *
    &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer()) 
    &gt;&gt;&gt; l2, gradients = solver.compute_gradients()
    &gt;&gt;&gt; solver.adjust_weights(gradients)
    {&#39;zero_boundary&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=38.457893&gt;, &#39;f_boundary&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=22.964037&gt;, &#39;inner&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;}
    &#34;&#34;&#34;

    gradient_vectors = [tf.concat([tf.reshape(gradient, [-1]) for gradient in gradients_list], axis=0) for gradients_list in gradients.values()][:-1] # excluding &#39;totalgrad&#39;
    variances = [tf.math.reduce_variance(gradient) for gradient in gradient_vectors]
    most_varying = tf.math.argmax(variances)
    most_varying_absmax = tf.math.reduce_max(tf.abs(tf.gather(gradient_vectors, most_varying)))

    new_weights = {}
    minimal = tf.float32.max
    for i, cond in enumerate(self.bvp.get_conditions()):
        name = cond.name
        if i != most_varying:
            new_weight = most_varying_absmax / (tf.reduce_mean(tf.abs(gradient_vectors[i])))
            new_weight = 0.9 * self.weights[i] + 0.1 * new_weight
        else:
            new_weight = self.weights[i]

        new_weights[name] = new_weight
        minimal = tf.math.minimum(minimal, new_weights[name])

    new_weights = {name: new_weights[name] / minimal for name in new_weights.keys()}
    self.weights.assign([new_weight for new_weight in new_weights.values()])
            
    return new_weights</code></pre>
</details>
</dd>
<dt id="PDESolver.Solver.Solver.compute_differentials"><code class="name flex">
<span>def <span class="ident">compute_differentials</span></span>(<span>self, samplePoints)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the differentials of the model at the given points.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>samplePoints</code></strong> :&ensp;<code>tensor</code></dt>
<dd>Tensor of points to calculate the differentials at</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong> :&ensp;<code>Dictionary containing the differentials</code> of <code>the model needed for the bvp at the given points</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from PDESolver import *
&gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
&gt;&gt;&gt; samples = tf.constant([[0, 0], [0, 0.5], [0, 1]])  
&gt;&gt;&gt; solver.compute_differentials(samples) 
{'x': &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.],
    [0.],
    [0.]], dtype=float32)&gt;, 'y': &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0. ],
    [0.5],
    [1. ]], dtype=float32)&gt;, 'u': &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[-1.4298753 ],
    [-1.0829226 ],
    [-0.86016774]], dtype=float32)&gt;, 'u_x': &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.7101533 ],
    [0.7047943 ],
    [0.45764494]], dtype=float32)&gt;, 'u_xx': &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[-1.6525286],
    [-1.6283079],
    [-1.2708694]], dtype=float32)&gt;, 'u_y': &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.71874154],
    [0.6189531 ],
    [0.2856549 ]], dtype=float32)&gt;, 'u_yy': &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[-0.01344297],
    [-0.5869958 ],
    [-0.47277603]], dtype=float32)&gt;}
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_differentials(self, samplePoints):
    &#34;&#34;&#34;
    Calculates the differentials of the model at the given points.

    Parameters
    -----------
    samplePoints: tensor
        Tensor of points to calculate the differentials at

    Returns
    -----------
    dict: Dictionary containing the differentials of the model needed for the bvp at the given points

    Examples
    -----------
    &gt;&gt;&gt; from PDESolver import *
    &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
    &gt;&gt;&gt; samples = tf.constant([[0, 0], [0, 0.5], [0, 1]])  
    &gt;&gt;&gt; solver.compute_differentials(samples) 
    {&#39;x&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
    array([[0.],
        [0.],
        [0.]], dtype=float32)&gt;, &#39;y&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
    array([[0. ],
        [0.5],
        [1. ]], dtype=float32)&gt;, &#39;u&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
    array([[-1.4298753 ],
        [-1.0829226 ],
        [-0.86016774]], dtype=float32)&gt;, &#39;u_x&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
    array([[0.7101533 ],
        [0.7047943 ],
        [0.45764494]], dtype=float32)&gt;, &#39;u_xx&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
    array([[-1.6525286],
        [-1.6283079],
        [-1.2708694]], dtype=float32)&gt;, &#39;u_y&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
    array([[0.71874154],
        [0.6189531 ],
        [0.2856549 ]], dtype=float32)&gt;, &#39;u_yy&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
    array([[-0.01344297],
        [-0.5869958 ],
        [-0.47277603]], dtype=float32)&gt;}
    &#34;&#34;&#34;

    specs = self.bvp.get_specification()

    stack = lambda *tensors: tf.concat(tensors, axis=1)
    component_funs = {component: lambda x, index=i: self.model(x)[:, index] 
                      for i, component in enumerate(specs[&#34;components&#34;])}

    gradient_dict = {}

    with tf.GradientTape(persistent=True) as tape:
        for i, variable in enumerate(specs[&#34;variables&#34;]):
            gradient_dict[variable] = samplePoints[:, i:i + 1]
            tape.watch(gradient_dict[variable])

        watched = [gradient_dict[variable] for variable in specs[&#34;variables&#34;]]
        for component in specs[&#34;components&#34;]:
            output = component_funs[component](tf.stack(watched, axis=1))
            gradient_dict[component] = tf.reshape(output, (-1, 1))

        for differential in specs[&#34;differentials&#34;]:
            component = differential.split(&#34;_&#34;)[0]
            variables = differential.split(&#34;_&#34;)[1]

            differential_head = component 
            for i, variable in enumerate(variables):
                if i == 0:
                    differential_head_new = differential_head + &#34;_&#34; + variable
                else:
                    differential_head_new = differential_head + variable

                if not differential_head_new in gradient_dict:
                    gradient_dict[differential_head_new] = tape.gradient(gradient_dict[differential_head], gradient_dict[variable])
                    differential_head = differential_head_new

    for (stacked_component_name, components) in specs[&#34;stacked_components&#34;].items():
        stacked = stack(*[gradient_dict[component] for component in components])
        gradient_dict[stacked_component_name] = stacked

    return gradient_dict</code></pre>
</details>
</dd>
<dt id="PDESolver.Solver.Solver.compute_gradients"><code class="name flex">
<span>def <span class="ident">compute_gradients</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the gradients of the neural network.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>tensor</code></strong> :&ensp;<code>L2 loss</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>dict</code></strong> :&ensp;<code>Dictionary</code> of <code>gradients</code> of <code>form {condition_name: gradient}</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from PDESolver import *
&gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
&gt;&gt;&gt; solver.compute_gradients()
(&lt;tf.Tensor: shape=(), dtype=float32, numpy=4.756295&gt;, LARGE DICTIONARY)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_gradients(self):
    &#34;&#34;&#34;
    Gets the gradients of the neural network.

    Returns
    -----------
    tensor: L2 loss
    dict: Dictionary of gradients of form {condition_name: gradient}

    Examples
    -----------
    &gt;&gt;&gt; from PDESolver import *
    &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
    &gt;&gt;&gt; solver.compute_gradients()
    (&lt;tf.Tensor: shape=(), dtype=float32, numpy=4.756295&gt;, LARGE DICTIONARY)
    &#34;&#34;&#34;
    
    with tf.GradientTape(persistent=True) as tape:
        tape.watch(self.model.trainable_variables)
        losses = self.compute_losses()

        kwargs = {&#39;sources&#39;: self.model.trainable_variables, &#39;unconnected_gradients&#39;: tf.UnconnectedGradients.ZERO}
        grads = {name: tape.gradient(loss, **kwargs) for name, loss in losses.items() if name != &#39;L2&#39;}

    return losses[&#39;L2&#39;], grads</code></pre>
</details>
</dd>
<dt id="PDESolver.Solver.Solver.compute_losses"><code class="name flex">
<span>def <span class="ident">compute_losses</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the losses for the neural network.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong> :&ensp;<code>Dictionary</code> of <code>losses</code> of <code>form {condition_name: loss}</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from PDESolver import *
&gt;&gt;&gt; solver = Solver(Laplace(), Optimizer()) 
&gt;&gt;&gt; solver.compute_losses() 
{'zero_boundary': &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.29857153&gt;, 'f_boundary': &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.07588613&gt;, 'inner': &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.3834033&gt;, 'L2': &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.757861&gt;, 'total': &lt;tf.Tensor: shape=(), dtype=float32, numpy=13.515722&gt;}
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_losses(self):
    &#34;&#34;&#34;
    Computes the losses for the neural network.
    
    Returns
    -----------
    dict: Dictionary of losses of form {condition_name: loss}

    Examples
    -----------
    &gt;&gt;&gt; from PDESolver import *
    &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer()) 
    &gt;&gt;&gt; solver.compute_losses() 
    {&#39;zero_boundary&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.29857153&gt;, &#39;f_boundary&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.07588613&gt;, &#39;inner&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.3834033&gt;, &#39;L2&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.757861&gt;, &#39;total&#39;: &lt;tf.Tensor: shape=(), dtype=float32, numpy=13.515722&gt;}
    &#34;&#34;&#34;

    criterion = tf.keras.losses.MeanSquaredError()
    residuals = self.compute_residuals()

    losses = {name: 0.0 for name in residuals.keys()}
    l2loss = 0.0
    for i, (name, residual) in enumerate(residuals.items()):
        losses[name] += self.weights[i] * criterion(residual, 0.0)
        l2loss += criterion(residual, 0.0)

    losses[&#39;total&#39;] = sum(losses.values())
    losses[&#39;L2&#39;] = l2loss

    return losses</code></pre>
</details>
</dd>
<dt id="PDESolver.Solver.Solver.compute_residuals"><code class="name flex">
<span>def <span class="ident">compute_residuals</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the residuals for the boundary value problem.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong> :&ensp;<code>Dictionary</code> of <code>residuals</code> of <code>form {condition_name: residual}</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from PDESolver import *
&gt;&gt;&gt; solver = Solver(Laplace(minibatch_size=3), Optimizer()) 
&gt;&gt;&gt; solver.compute_residuals()
{'zero_boundary': &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[1.7787822],
    [2.6922252],
    [1.3558891]], dtype=float32)&gt;, 'f_boundary': &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[2.4186134],
    [2.4582624],
    [2.4142814]], dtype=float32)&gt;, 'inner': &lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=
array([[ 0.53346014],
    [-4.9377484 ],
    [ 0.21452093],
    [-1.6314204 ]], dtype=float32)&gt;}
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_residuals(self):
    &#34;&#34;&#34;
    Gets the residuals for the boundary value problem.

    Returns
    -----------
    dict: Dictionary of residuals of form {condition_name: residual}

    Examples
    -----------
    &gt;&gt;&gt; from PDESolver import *
    &gt;&gt;&gt; solver = Solver(Laplace(minibatch_size=3), Optimizer()) 
    &gt;&gt;&gt; solver.compute_residuals()
    {&#39;zero_boundary&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
    array([[1.7787822],
        [2.6922252],
        [1.3558891]], dtype=float32)&gt;, &#39;f_boundary&#39;: &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
    array([[2.4186134],
        [2.4582624],
        [2.4142814]], dtype=float32)&gt;, &#39;inner&#39;: &lt;tf.Tensor: shape=(4, 1), dtype=float32, numpy=
    array([[ 0.53346014],
        [-4.9377484 ],
        [ 0.21452093],
        [-1.6314204 ]], dtype=float32)&gt;}
    &#34;&#34;&#34;

    conditions = self.bvp.get_conditions()

    residuals = {}
    for condition in conditions:
        samples = condition.sample_points()
        Du = self.compute_differentials(samples)
        residuals[condition.name] = condition.residue_fn(Du)

    return residuals</code></pre>
</details>
</dd>
<dt id="PDESolver.Solver.Solver.show_debugplot"><code class="name flex">
<span>def <span class="ident">show_debugplot</span></span>(<span>self, gradients)</span>
</code></dt>
<dd>
<div class="desc"><p>Shows a debug plot of the neural network.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>gradients</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Tuple of gradients of the PDE and data losses obtained from compute_gradients</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from PDESolver import *                        
&gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
&gt;&gt;&gt; l2loss, gradients = solver.compute_gradients()
&gt;&gt;&gt; solver.loss_history += [l2loss.numpy()]
&gt;&gt;&gt; solver.show_debugplot(gradients)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_debugplot(self, gradients):
    &#34;&#34;&#34;
    Shows a debug plot of the neural network.

    Parameters
    -----------
    gradients: tuple
        Tuple of gradients of the PDE and data losses obtained from compute_gradients

    Examples
    -----------
    &gt;&gt;&gt; from PDESolver import *                        
    &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
    &gt;&gt;&gt; l2loss, gradients = solver.compute_gradients()
    &gt;&gt;&gt; solver.loss_history += [l2loss.numpy()]
    &gt;&gt;&gt; solver.show_debugplot(gradients)    
    &#34;&#34;&#34;

    fig = plt.figure(figsize=(16, 8), layout=&#39;compressed&#39;)
    subfigs = fig.subfigures(2, 1, hspace=0)

    axs = subfigs[0].subplots(1, len(self.model.layers) - 4)
    subfigs[0].suptitle(&#39;Gradient Distributions&#39;)

    ax_count = -1
    for j in range(len(self.model.layers)):
        if j in [0, 1, 3, 4]:
            continue
        else:
            ax_count += 1

        layer_gradient = lambda name: tf.concat([tf.reshape(gradient, [-1]) for gradient in gradients[name][2*(j-2):2*(j-1)]], axis=0)
        density = lambda i, x: gaussian_kde(layer_gradient(i).numpy())(x)

        xs = np.linspace(-2.5, 2.5, 1000)
        for cond in self.bvp.get_conditions():
            axs[ax_count].plot(xs, density(cond.name, xs), lw=0.5, label=cond.name)

        axs[ax_count].set_xlim(min(xs), max(xs))
        axs[ax_count].set_ylim(0, 100)
        axs[ax_count].set_yscale(&#39;symlog&#39;)
        axs[ax_count].set_title(self.model.layers[j].name)

    axs[-1].legend()

    n = len(self.loss_history)
    averaged_loss = [np.mean(self.loss_history[:k+1] if k &lt; 99 else self.loss_history[k-99:k+1]) for k in range(n)]
    best_loss = np.min(self.loss_history)

    axs = subfigs[1].subplots(1, 2)
    trans = blended_transform_factory(axs[0].transAxes, axs[0].transData)
    loss_handle, = axs[0].semilogy(range(n), self.loss_history, &#39;k-&#39;, lw=0.5, alpha=0.5, label=&#39;Loss&#39;)
    avg_loss_handle, = axs[0].semilogy(range(n), averaged_loss, &#39;r--&#39;, lw=1, label=f&#39;$ø_{{{min(100, n)}}}$ Loss&#39;)
    axs[0].axhline(best_loss, color=&#39;g&#39;, lw=0.5)
    axs[0].text(0.995, 0.97 * best_loss, f&#39;Best: {best_loss:.3e}&#39;, ha=&#39;right&#39;, va=&#39;top&#39;, color=&#39;g&#39;, transform=trans)

    ax = axs[0].twinx()
    ax.set_yscale(&#39;log&#39;)
    lr_handle, = ax.plot(self.optimizer.lr_history_upto(n), &#39;b-&#39;, lw=0.5, label=&#39;Learning rate&#39;)

    axs[0].set_title(&#39;Loss (left) and learning rate (right) history&#39;)
    axs[0].set_xlabel(&#39;Iteration&#39;)
    axs[0].set_xlim(0, n - (1 if n &gt; 1 else 0))

    if n &gt; 1:
        handles = [loss_handle, avg_loss_handle, lr_handle]
        axs[0].legend(handles=handles, loc=&#39;lower left&#39;)

    for cond in self.bvp.get_conditions():
        cond_weight_history = [self.weight_history[k][cond.name] for k in range(len(self.weight_history))]
        axs[1].plot(cond_weight_history, lw=0.5)

    axs[1].set_title(&#39;Weight History&#39;)
    axs[1].set_xlabel(&#39;Update #&#39;)
    axs[1].set_xlim(0, len(self.weight_history) - 1)
    axs[1].set_yscale(&#39;log&#39;)

    if os.name == &#39;nt&#39;:
        figManager = plt.get_current_fig_manager()
        figManager.window.state(&#34;zoomed&#34;)
    
    plt.show()</code></pre>
</details>
</dd>
<dt id="PDESolver.Solver.Solver.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, iterations=10000, debug_frequency=2500)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the neural network to solve the boundary value problem.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>iterations</code></strong> :&ensp;<code>int (default=10000)</code></dt>
<dd>Number of iterations to train for</dd>
<dt><strong><code>debug_frequency</code></strong> :&ensp;<code>int (default=2500)</code></dt>
<dd>Frequency (every X iterations) at which to show debug panel.
If negative, no debug panel is shown</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from PDESolver import *
&gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
&gt;&gt;&gt; solver.train(iterations=10000, debug_frequency=2500)
øL²-loss = 6.044e+01 (best: 2.305e+01, 000064it ago) lr = 0.00090:   3%|█                                | 1271/40000 [00:33&lt;07:53, 81.76it/s]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, iterations=10000, debug_frequency=2500):
    &#34;&#34;&#34;
    Trains the neural network to solve the boundary value problem.

    Parameters
    -----------
    iterations: int (default=10000)
        Number of iterations to train for
    debug_frequency: int (default=2500)
        Frequency (every X iterations) at which to show debug panel.
        If negative, no debug panel is shown

    Examples
    -----------
    &gt;&gt;&gt; from PDESolver import *
    &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
    &gt;&gt;&gt; solver.train(iterations=10000, debug_frequency=2500)
    øL²-loss = 6.044e+01 (best: 2.305e+01, 000064it ago) lr = 0.00090:   3%|█                                | 1271/40000 [00:33&lt;07:53, 81.76it/s]
    &#34;&#34;&#34;

    best_loss = np.inf
    iterations_since_last_improvement = 0
    k_max = int(np.ceil(np.log10(iterations))) + 1
    pbar = tqdm(range(iterations), desc=&#39;Pending...&#39;)

    for i in pbar:
        l2loss, gradients, new_weights = self.train_step()
        
        self.loss_history += [l2loss.numpy()]
        
        if l2loss.numpy() &lt; best_loss:
            best_loss = l2loss.numpy()
            iterations_since_last_improvement = 0
        else:
            iterations_since_last_improvement += 1
        
        if list(new_weights.values())[0] != -1:
            self.weight_history += [{name: new_weights[name].numpy() for name in new_weights.keys()}]
        
        avg_loss = np.mean(self.loss_history[-100:])
        pbar.desc = f&#39;øL²-loss = {avg_loss:.3e} (best: {best_loss:.3e}, {iterations_since_last_improvement:0{k_max}d}it ago) lr = {self.optimizer.lr.numpy():.5f}&#39;

        if debug_frequency &gt; 0 and (i % debug_frequency == 0 or i == iterations - 1):
            self.show_debugplot(gradients)</code></pre>
</details>
</dd>
<dt id="PDESolver.Solver.Solver.train_step"><code class="name flex">
<span>def <span class="ident">train_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs a single training step.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>tensor</code></strong> :&ensp;<code>L2 loss</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>dict</code></strong> :&ensp;<code>Dictionary</code> of <code>gradients</code> of <code>form {condition_name: gradient}</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>dict</code></strong> :&ensp;<code>Dictionary</code> of <code>adjusted condition weights</code> of <code>form {condition_name: weight}</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from PDESolver import *
&gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
&gt;&gt;&gt; l2loss, gradients, new_weights = solver.train_step()
&gt;&gt;&gt; l2loss
&lt;tf.Tensor: shape=(), dtype=float32, numpy=4.371607&gt;
&gt;&gt;&gt; l2loss, gradients, new_weights = solver.train_step()
&gt;&gt;&gt; l2loss
&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0963678&gt;
&gt;&gt;&gt; l2loss, gradients, new_weights = solver.train_step()
&gt;&gt;&gt; l2loss
&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.029224&gt;
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@tf.function
def train_step(self):
    &#34;&#34;&#34;
    Performs a single training step.

    Returns
    -----------
    tensor: L2 loss
    dict: Dictionary of gradients of form {condition_name: gradient}
    dict: Dictionary of adjusted condition weights of form {condition_name: weight}

    Examples
    -----------
    &gt;&gt;&gt; from PDESolver import *
    &gt;&gt;&gt; solver = Solver(Laplace(), Optimizer())
    &gt;&gt;&gt; l2loss, gradients, new_weights = solver.train_step()
    &gt;&gt;&gt; l2loss
    &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.371607&gt;
    &gt;&gt;&gt; l2loss, gradients, new_weights = solver.train_step()
    &gt;&gt;&gt; l2loss
    &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0963678&gt;
    &gt;&gt;&gt; l2loss, gradients, new_weights = solver.train_step()
    &gt;&gt;&gt; l2loss
    &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.029224&gt;
    &#34;&#34;&#34;
    
    l2loss, gradients = self.compute_gradients()
    
    if self.step % 10 == 0:
        new_weights = self.adjust_weights(gradients)  
    else:
        new_weights = {cond.name: -1. for cond in self.bvp.get_conditions()}          
        
    self.optimizer.apply_gradients(zip(gradients[&#39;total&#39;], self.model.trainable_variables))
    self.step.assign(self.step + 1)
    
    return l2loss, gradients, new_weights</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="PDESolver" href="index.html">PDESolver</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="PDESolver.Solver.Optimizer" href="#PDESolver.Solver.Optimizer">Optimizer</a></code></h4>
<ul class="">
<li><code><a title="PDESolver.Solver.Optimizer.lr_history_upto" href="#PDESolver.Solver.Optimizer.lr_history_upto">lr_history_upto</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PDESolver.Solver.Solver" href="#PDESolver.Solver.Solver">Solver</a></code></h4>
<ul class="">
<li><code><a title="PDESolver.Solver.Solver.adjust_weights" href="#PDESolver.Solver.Solver.adjust_weights">adjust_weights</a></code></li>
<li><code><a title="PDESolver.Solver.Solver.compute_differentials" href="#PDESolver.Solver.Solver.compute_differentials">compute_differentials</a></code></li>
<li><code><a title="PDESolver.Solver.Solver.compute_gradients" href="#PDESolver.Solver.Solver.compute_gradients">compute_gradients</a></code></li>
<li><code><a title="PDESolver.Solver.Solver.compute_losses" href="#PDESolver.Solver.Solver.compute_losses">compute_losses</a></code></li>
<li><code><a title="PDESolver.Solver.Solver.compute_residuals" href="#PDESolver.Solver.Solver.compute_residuals">compute_residuals</a></code></li>
<li><code><a title="PDESolver.Solver.Solver.show_debugplot" href="#PDESolver.Solver.Solver.show_debugplot">show_debugplot</a></code></li>
<li><code><a title="PDESolver.Solver.Solver.train" href="#PDESolver.Solver.Solver.train">train</a></code></li>
<li><code><a title="PDESolver.Solver.Solver.train_step" href="#PDESolver.Solver.Solver.train_step">train_step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>